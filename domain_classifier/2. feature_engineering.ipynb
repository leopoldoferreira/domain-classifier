{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "vowel = 'aeiou'\n",
    "numbers = '0123456789'\n",
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "def pairs_entropy(domain):\n",
    "        # Extract the number of consecutive vowels, numbers and consonants\n",
    "        count_pair_vowel = 0\n",
    "        count_pair_consonants = 0\n",
    "        count_pair_numbers = 0\n",
    "        # Strip domain\n",
    "        domain_stripped = domain.strip()\n",
    "        # If find a consecutive vowel/number/consonant, increase counter\n",
    "        for index in range(len(domain_stripped)-1):\n",
    "            if (domain_stripped[index] in vowel and\n",
    "                    domain_stripped[index+1] in vowel):\n",
    "                count_pair_vowel += 1\n",
    "            if (domain_stripped[index] in consonants and\n",
    "                    domain_stripped[index+1] in consonants):\n",
    "                count_pair_consonants += 1\n",
    "            if (domain_stripped[index] in numbers and\n",
    "                    domain_stripped[index+1] in numbers):\n",
    "                count_pair_numbers += 1\n",
    "        # Shannon Entropy quantifies the amount of information in a variable\n",
    "        character_frequency = {}\n",
    "        entropy = 0\n",
    "        lenght = len(domain)\n",
    "        for character in domain:\n",
    "            character_frequency[character] = domain.count(character)\n",
    "        for character in character_frequency:\n",
    "            probability_character = float(\n",
    "                character_frequency[character]) / float(lenght)\n",
    "            entropy -= probability_character*math.log(probability_character, 2)\n",
    "        return (count_pair_vowel, count_pair_consonants,\n",
    "                count_pair_numbers, round(entropy, 3))\n",
    "\n",
    "\n",
    "def lexical_features_extractor(_list, output):\n",
    "    # Extraction of lexical features, \n",
    "    df = pd.DataFrame(_list,columns=['domain'])\n",
    "    # Self explanatory name columns to DataFrame df\n",
    "    df['length'] = df['domain'].str.len()\n",
    "    df['vowel'] = df['domain'].str.count(r'[aeiou]')\n",
    "    df['vowel_rate'] = round(df.vowel / df.length, 3)\n",
    "    df['consonant'] = df['domain'].str.count(r'[a-z]') - df['vowel']\n",
    "    df['consonant_rate'] = round(df.consonant / df.length, 3)\n",
    "    df['number'] = df['domain'].str.count(r'[0-9]')\n",
    "    df['num_rate'] = round(df.number / df.length, 3)\n",
    "    df['dash'] = df['domain'].str.count(r'[-]')\n",
    "    df['dash_rate'] = round(df.dash / df.length, 3)\n",
    "    # Concatenate pairs_entropy features\n",
    "    np_consec = np.zeros(shape=(len(df), 4))\n",
    "    for index in df.itertuples():\n",
    "        np_consec[index.Index] = pairs_entropy(index.domain)\n",
    "    consec = pd.DataFrame.from_records(np_consec)\n",
    "    consec.columns = ['vowel_pair', 'consonant_pair', 'number_pair', 'entropy']\n",
    "    df = pd.concat([df, consec], axis=1)\n",
    "    \n",
    "    pickle.dump(df, open(abspath('./data/features/' + output), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import lists\n",
    "legitimates = [line.rstrip('\\n') for line in open(\n",
    "    abspath('./data/lists/cleaned/legitimate_cleaned'))]\n",
    "malicious = [line.rstrip('\\n') for line in open(\n",
    "    abspath('./data/lists/cleaned/malicious_cleaned'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run method to extract lexical features from public legitimate domains\n",
    "lexical_features_extractor(legitimates, 'public_leg')\n",
    "\n",
    "# Transform into a DataFrame\n",
    "legitimates = pd.read_pickle(abspath('./data/features/public_leg'))\n",
    "# Remove domain name\n",
    "legitimates.pop('domain')\n",
    "# Add target\n",
    "legitimates.insert(loc=0, column='class', value=0)\n",
    "\n",
    "# Same thing for the public malicious domains\n",
    "lexical_features_extractor(malicious, 'public_mal')\n",
    "malicious = pd.read_pickle(abspath('./data/features/public_mal'))\n",
    "malicious.pop('domain')\n",
    "malicious.insert(loc=0, column='class', value=1)\n",
    "\n",
    "\n",
    "public_lexical = pd.concat([legitimates, malicious])\n",
    "public_lexical.dropna(inplace = True)\n",
    "public_lexical.to_csv('./data/features/public_lexical.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
